{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network and training\n",
    "nb_epoch = 200\n",
    "batch_size = 128\n",
    "verbose = 1\n",
    "nb_classes = 10 # number of outputs = number of digits\n",
    "optimizer = SGD() #sgd optimizer\n",
    "n_hidden = 128\n",
    "validation_split = 0.2 # percentage for validation\n",
    "dropout=0.3\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "reshaped = 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(60000, reshaped)\n",
    "x_test = x_test.reshape(10000, reshaped)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /=255\n",
    "x_test /=255\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(nb_classes, input_shape=(784,)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the optimizer is the specific\n",
    "# algorithm used to update weights while we train our model\n",
    "# objective function that is used by the optimizer to navigate\n",
    "# the space of weights\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE: \n",
    "This is the mean squared error between the predictions and the true values.\n",
    "Mathematically, if is a vector of n predictions, and Y is the vector of n observed\n",
    "values, then they satisfy the following equation:\n",
    "\n",
    "\n",
    "These objective functions average all the mistakes made for each\n",
    "prediction, and if the prediction is far from the true value, then this\n",
    "distance is made more evident by the squaring operation.\n",
    "### Binary cross-entropy: \n",
    "This is the binary logarithmic loss. Suppose that our model\n",
    "predicts p while the target is t, then the binary cross-entropy is defined as follows:\n",
    "\n",
    "\n",
    "This objective function is suitable for binary labels prediction.\n",
    "### Categorical cross-entropy: \n",
    "This is the multiclass logarithmic loss. If the target is\n",
    "ti,j and the prediction is pi,j, then the categorical cross-entropy is this:\n",
    "\n",
    "\n",
    "This objective function is suitable for multiclass labels predictions. It is\n",
    "also the default choice in association with softmax activation.\n",
    "\n",
    "\n",
    "sparse_categorical_accuracy  (useful for sparse targets), and\n",
    "top_k_categorical_accuracy (success when the target class is within the\n",
    "top_k predictions provided).\n",
    "### Accuracy:\n",
    "This is the proportion of correct predictions with respect to the targets\n",
    "### Precision: \n",
    "This denotes how many selected items are relevant for a multilabel\n",
    "classification\n",
    "### Recall: \n",
    "This denotes how many selected items are relevant for a multilabel\n",
    "classification\n",
    "### epochs: \n",
    "This is the number of times the model is exposed to the training set. At\n",
    "each iteration, the optimizer tries to adjust the weights so that the objective\n",
    "function is minimized.\n",
    "### batchsize: \n",
    "This is the number of training instances observed before the\n",
    "optimizer performs a weight update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 13s 271us/step - loss: 2.3019 - acc: 0.1159 - val_loss: 2.3018 - val_acc: 0.10606s - loss: 2.3 - ETA: 3s - loss:\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 7s 145us/step - loss: 2.3012 - acc: 0.1140 - val_loss: 2.3016 - val_acc: 0.1060\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 9s 181us/step - loss: 2.3009 - acc: 0.1140 - val_loss: 2.3015 - val_acc: 0.1060\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 6s 133us/step - loss: 2.3007 - acc: 0.1140 - val_loss: 2.3015 - val_acc: 0.1060- loss: 2.\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 8s 170us/step - loss: 2.3006 - acc: 0.1140 - val_loss: 2.3015 - val_acc: 0.1060\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 7s 145us/step - loss: 2.3005 - acc: 0.1140 - val_loss: 2.3015 - val_acc: 0.1060000 - acc: 0. - ETA: 4s - loss: 2.3002 - acc: 0.1 - ETA: 4s - lo - ETA: 3s - loss: 2.3004 - acc - ETA: 2s - loss: 2.3003 - acc: 0 - ETA: 2s - loss: 2. - ETA: 1s - loss: 2.\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 11s 231us/step - loss: 2.3004 - acc: 0.1140 - val_loss: 2.3014 - val_acc: 0.1060s - loss: 2.3006 - acc: 0.1 - \n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 12s 258us/step - loss: 2.2999 - acc: 0.1140 - val_loss: 2.3010 - val_acc: 0.1060: 2.300\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 16s 340us/step - loss: 2.2998 - acc: 0.1140 - val_loss: 2.3009 - val_acc: 0.1060ss: 2.3 - ETA: 1s - loss: 2.2999 - acc: 0.113 - ETA: 1s - loss: 2.2999 - \n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 11s 234us/step - loss: 2.2998 - acc: 0.1140 - val_loss: 2.3008 - val_acc: 0.1060\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 12s 256us/step - loss: 2.2997 - acc: 0.1140 - val_loss: 2.3008 - val_acc: 0.1060- loss: 2.2993 - acc: 0.1 - ETA: 7s  - ETA: 6s - loss: 2.2995 - acc: 0 - ETA: 5s - loss: 2 - ETA: 4s - loss: 2.2998 - acc: 0.11 - ETA: 4s - loss: 2.2998 - acc: 0.11 - ETA: 4s - loss: 2.2999 - ETA: 2s - - ETA: 0s - loss: 2.2997 - acc: 0.114 - ETA: 0s - loss: 2.2997 - acc:\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 12s 244us/step - loss: 2.2996 - acc: 0.1140 - val_loss: 2.3007 - val_acc: 0.1060114 - ETA: 1s - loss - ETA: 0s - loss: 2.2997 - acc: \n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 14s 282us/step - loss: 2.2986 - acc: 0.1140 - val_loss: 2.2996 - val_acc: 0.1060980 - a - ETA: 6s - loss: 2.2980 - acc: 0.1 - ETA: 6s - loss: 2.2981 - acc: 0.11 - ETA: 5s - loss: 2.2981 - acc: 0 - ETA: 4s - loss: 2.2981  - ETA: 3s  - ETA: 0s - loss: 2.2985 - acc: 0\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 13s 274us/step - loss: 2.2985 - acc: 0.1140 - val_loss: 2.2995 - val_acc: 0.10602.2992 - acc: 0 - ETA\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 14s 299us/step - loss: 2.2984 - acc: 0.1140 - val_loss: 2.2995 - val_acc: 0.1060s - loss: 2.2986 - acc:  - ETA: 7s - loss: 2.2986 - acc: 0.113 - ETA: 7s - ETA: 5s - l - ETA: 0s - loss: 2.2985 - acc\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 24s 501us/step - loss: 2.2972 - acc: 0.1140 - val_loss: 2.2983 - val_acc: 0.1060\n",
      "Epoch 58/200\n",
      "10240/48000 [=====>........................] - ETA: 20s - loss: 2.2964 - acc: 0.1180"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bunmi\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.118947). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Bunmi\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.124932). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 30s 635us/step - loss: 2.2972 - acc: 0.1140 - val_loss: 2.2982 - val_acc: 0.1060\n",
      "Epoch 59/200\n",
      "  128/48000 [..............................] - ETA: 31s - loss: 2.2910 - acc: 0.1172"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bunmi\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.383989). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  640/48000 [..............................] - ETA: 1:12 - loss: 2.2979 - acc: 0.1094"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bunmi\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.361991). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Bunmi\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.339993). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Bunmi\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.179999). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1792/48000 [>.............................] - ETA: 56s - loss: 2.2959 - acc: 0.1161 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bunmi\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.123984). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7296/48000 [===>..........................] - ETA: 34s - loss: 2.2963 - acc: 0.1172 ETA: 31s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bunmi\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.159900). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 29s 613us/step - loss: 2.2971 - acc: 0.1140 - val_loss: 2.2981 - val_acc: 0.1060TA: 3s - loss: 2.2971 - acc: - ETA: 2s -\n",
      "Epoch 60/200\n",
      "  768/48000 [..............................] - ETA: 18s - loss: 2.2959 - acc: 0.1224"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bunmi\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.143990). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 16s 324us/step - loss: 2.2970 - acc: 0.1140 - val_loss: 2.2980 - val_acc: 0.1060\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 27s 573us/step - loss: 2.2964 - acc: 0.1140 - val_loss: 2.2974 - val_acc: 0.1060 8s \n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 34s 718us/step - loss: 2.2963 - acc: 0.1140 - val_loss: 2.2973 - val_acc: 0.1060\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 17s 363us/step - loss: 2.2963 - acc: 0.1140 - val_loss: 2.2973 - val_acc: 0.1060- ETA: 1s - loss: 2.2963 - acc: 0. - ETA: 0s - loss: 2.2963 - acc:  - ETA: 0s - loss: 2.2963 - acc: 0.11\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 17s 363us/step - loss: 2.2962 - acc: 0.1140 - val_loss: 2.2972 - val_acc: 0.1060\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 27s 563us/step - loss: 2.2962 - acc: 0.1140 - val_loss: 2.2972 - val_acc: 0.1060\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 22s 459us/step - loss: 2.2961 - acc: 0.1140 - val_loss: 2.2971 - val_acc: 0.1060\n",
      "Epoch 76/200\n",
      "  384/48000 [..............................] - ETA: 34s - loss: 2.2989 - acc: 0.0911"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bunmi\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.100793). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 20s 417us/step - loss: 2.2960 - acc: 0.1140 - val_loss: 2.2970 - val_acc: 0.1060\n",
      "Epoch 77/200\n",
      "  896/48000 [..............................] - ETA: 15s - loss: 2.2964 - acc: 0.1060"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bunmi\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.111971). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 17s 359us/step - loss: 2.2960 - acc: 0.1140 - val_loss: 2.2969 - val_acc: 0.1060\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 18s 384us/step - loss: 2.2959 - acc: 0.1140 - val_loss: 2.2969 - val_acc: 0.1060 acc: 0.11 - ETA: 2s - loss: 2.2959 - acc: 0.1 - ETA: 2s - loss: 2 - ETA: 0s - loss: 2.2958 -  - ETA: 0s - loss: 2.2959 - acc: 0.114\n",
      "Epoch 79/200\n",
      "17792/48000 [==========>...................] - ETA: 12s - loss: 2.2957 - acc: 0.1158 ETA: 12s - loss: 2.2957 - acc: 0.11"
     ]
    }
   ],
   "source": [
    "# model is trained with fit() after compiling\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "                   verbose=verbose, validation_split=validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 486us/step\n",
      "Test score:  2.28786814956665\n",
      "Test accuracy:  0.1135\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=verbose)\n",
    "print('Test score: ', score[0])\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved net\n",
    "\n",
    "Start from adding additional hidden layers and an activation function relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 64s 1ms/step - loss: 2.3007 - acc: 0.1179 - val_loss: 2.3006 - val_acc: 0.1060loss: 2\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 18s 380us/step - loss: 2.2998 - acc: 0.1140 - val_loss: 2.3005 - val_acc: 0.1060\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 14s 298us/step - loss: 2.2995 - acc: 0.1140 - val_loss: 2.3003 - val_acc: 0.1060\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 17s 345us/step - loss: 2.2994 - acc: 0.1140 - val_loss: 2.3004 - val_acc: 0.1060\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 14s 285us/step - loss: 2.2993 - acc: 0.1140 - val_loss: 2.3003 - val_acc: 0.1060\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 14s 282us/step - loss: 2.2992 - acc: 0.1140 - val_loss: 2.3002 - val_acc: 0.1060\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 14s 290us/step - loss: 2.2992 - acc: 0.1140 - val_loss: 2.3002 - val_acc: 0.1060\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 13s 267us/step - loss: 2.2991 - acc: 0.1140 - val_loss: 2.3001 - val_acc: 0.1060\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 16s 323us/step - loss: 2.2990 - acc: 0.1140 - val_loss: 2.3000 - val_acc: 0.1060\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 15s 307us/step - loss: 2.2990 - acc: 0.1140 - val_loss: 2.3000 - val_acc: 0.1060\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 14s 299us/step - loss: 2.2989 - acc: 0.1140 - val_loss: 2.2999 - val_acc: 0.1060 1s - lo\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 14s 281us/step - loss: 2.2988 - acc: 0.1140 - val_loss: 2.2999 - val_acc: 0.1060\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 14s 301us/step - loss: 2.2987 - acc: 0.1140 - val_loss: 2.2997 - val_acc: 0.1060\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 13s 271us/step - loss: 2.2986 - acc: 0.1140 - val_loss: 2.2997 - val_acc: 0.1060\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 16s 329us/step - loss: 2.2985 - acc: 0.1140 - val_loss: 2.2995 - val_acc: 0.1060 4s - loss: 2.2 - ETA: 3s - loss - ETA: 1s - loss:\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 13s 269us/step - loss: 2.2985 - acc: 0.1140 - val_loss: 2.2994 - val_acc: 0.1060\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 12s 250us/step - loss: 2.2984 - acc: 0.1140 - val_loss: 2.2994 - val_acc: 0.1060\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 13s 275us/step - loss: 2.2983 - acc: 0.1140 - val_loss: 2.2992 - val_acc: 0.1060\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 12s 250us/step - loss: 2.2982 - acc: 0.1140 - val_loss: 2.2991 - val_acc: 0.1060\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 12s 250us/step - loss: 2.2981 - acc: 0.1140 - val_loss: 2.2991 - val_acc: 0.1060: 0.114 - ETA: 1s - \n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 12s 251us/step - loss: 2.2980 - acc: 0.1140 - val_loss: 2.2990 - val_acc: 0.1060: 2.2980 - acc: 0.\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 12s 246us/step - loss: 2.2979 - acc: 0.1140 - val_loss: 2.2989 - val_acc: 0.1060\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 12s 241us/step - loss: 2.2978 - acc: 0.1140 - val_loss: 2.2988 - val_acc: 0.1060A: 2s - loss: 2.2979 - - ETA: 1s\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 12s 247us/step - loss: 2.2977 - acc: 0.1140 - val_loss: 2.2987 - val_acc: 0.10600 - ETA: 2s - loss: 2.2978 - acc: - ETA: 2s - loss: 2.2978 - acc:  - ETA: 2s - loss - ETA: 0s - loss: 2.2976 - ac\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 12s 249us/step - loss: 2.2976 - acc: 0.1140 - val_loss: 2.2986 - val_acc: 0.1060\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 12s 246us/step - loss: 2.2975 - acc: 0.1140 - val_loss: 2.2986 - val_acc: 0.1060\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 12s 257us/step - loss: 2.2974 - acc: 0.1140 - val_loss: 2.2984 - val_acc: 0.1060\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 13s 268us/step - loss: 2.2973 - acc: 0.1140 - val_loss: 2.2983 - val_acc: 0.10602.2971 -  - ETA: 3s - loss: 2.2972 - acc: 0.11 - ETA: 3s - loss: 2.2972 - acc: 0.114 - ETA: 3s - loss: 2.2972 - ET\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 12s 247us/step - loss: 2.2972 - acc: 0.1140 - val_loss: 2.2982 - val_acc: 0.1060 - loss: 2.2975  - ETA: 0s - loss: 2.2972 - acc: 0.\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 12s 253us/step - loss: 2.2971 - acc: 0.1140 - val_loss: 2.2980 - val_acc: 0.1060 loss: 2.2\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 12s 253us/step - loss: 2.2970 - acc: 0.1140 - val_loss: 2.2980 - val_acc: 0.1060\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 12s 251us/step - loss: 2.2969 - acc: 0.1140 - val_loss: 2.2978 - val_acc: 0.1060\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 12s 250us/step - loss: 2.2967 - acc: 0.1140 - val_loss: 2.2977 - val_acc: 0.1060\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 12s 248us/step - loss: 2.2966 - acc: 0.1140 - val_loss: 2.2976 - val_acc: 0.1060\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 12s 251us/step - loss: 2.2965 - acc: 0.1140 - val_loss: 2.2974 - val_acc: 0.1060 - ETA: 0s - loss: 2.2965 - acc: \n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 12s 241us/step - loss: 2.2963 - acc: 0.1140 - val_loss: 2.2973 - val_acc: 0.1060 - acc: 0\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 11s 238us/step - loss: 2.2962 - acc: 0.1140 - val_loss: 2.2972 - val_acc: 0.1060loss: 2.2958  -\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 12s 248us/step - loss: 2.2961 - acc: 0.1140 - val_loss: 2.2970 - val_acc: 0.1060loss: 2\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 15s 303us/step - loss: 2.2959 - acc: 0.1140 - val_loss: 2.2970 - val_acc: 0.1060 - los\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 15s 318us/step - loss: 2.2958 - acc: 0.1140 - val_loss: 2.2969 - val_acc: 0.1060\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 13s 274us/step - loss: 2.2956 - acc: 0.1140 - val_loss: 2.2966 - val_acc: 0.1060\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 13s 279us/step - loss: 2.2955 - acc: 0.1140 - val_loss: 2.2964 - val_acc: 0.1060\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 12s 252us/step - loss: 2.2953 - acc: 0.1140 - val_loss: 2.2964 - val_acc: 0.10605 - acc - ETA: 0s - loss: 2.295\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 12s 247us/step - loss: 2.2952 - acc: 0.1140 - val_loss: 2.2962 - val_acc: 0.1060\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 12s 250us/step - loss: 2.2950 - acc: 0.1140 - val_loss: 2.2960 - val_acc: 0.1060\n",
      "Epoch 46/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 14s 301us/step - loss: 2.2948 - acc: 0.1140 - val_loss: 2.2958 - val_acc: 0.1060\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 14s 284us/step - loss: 2.2947 - acc: 0.1140 - val_loss: 2.2955 - val_acc: 0.1060\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 14s 295us/step - loss: 2.2945 - acc: 0.1140 - val_loss: 2.2955 - val_acc: 0.1060\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 15s 302us/step - loss: 2.2943 - acc: 0.1140 - val_loss: 2.2952 - val_acc: 0.10605s - loss: 2.2941 - acc: 0.1 - ET - ETA: 3s - loss - ETA: 1s - \n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 13s 276us/step - loss: 2.2941 - acc: 0.1140 - val_loss: 2.2950 - val_acc: 0.1060\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 12s 245us/step - loss: 2.2939 - acc: 0.1140 - val_loss: 2.2948 - val_acc: 0.1060\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 12s 255us/step - loss: 2.2937 - acc: 0.1140 - val_loss: 2.2946 - val_acc: 0.1060\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 12s 247us/step - loss: 2.2935 - acc: 0.1140 - val_loss: 2.2943 - val_acc: 0.1060- loss: 2. - ETA: 1s -\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 13s 276us/step - loss: 2.2933 - acc: 0.1140 - val_loss: 2.2941 - val_acc: 0.1060\n",
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 14s 289us/step - loss: 2.2931 - acc: 0.1140 - val_loss: 2.2939 - val_acc: 0.1060\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 15s 310us/step - loss: 2.2928 - acc: 0.1140 - val_loss: 2.2936 - val_acc: 0.1060\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 179s 4ms/step - loss: 2.2926 - acc: 0.1140 - val_loss: 2.2935 - val_acc: 0.1060\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 26s 538us/step - loss: 2.2923 - acc: 0.1140 - val_loss: 2.2932 - val_acc: 0.1060\n",
      "Epoch 59/200\n",
      "48000/48000 [==============================] - 19s 399us/step - loss: 2.2920 - acc: 0.1140 - val_loss: 2.2929 - val_acc: 0.1060\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 16s 332us/step - loss: 2.2918 - acc: 0.1140 - val_loss: 2.2927 - val_acc: 0.1060\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 19s 387us/step - loss: 2.2915 - acc: 0.1140 - val_loss: 2.2923 - val_acc: 0.1060\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 13s 267us/step - loss: 2.2912 - acc: 0.1140 - val_loss: 2.2921 - val_acc: 0.1060: 4s - lo\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 14s 293us/step - loss: 2.2909 - acc: 0.1140 - val_loss: 2.2919 - val_acc: 0.1060\n",
      "Epoch 64/200\n",
      "48000/48000 [==============================] - 12s 251us/step - loss: 2.2906 - acc: 0.1140 - val_loss: 2.2915 - val_acc: 0.1060\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 12s 249us/step - loss: 2.2903 - acc: 0.1140 - val_loss: 2.2912 - val_acc: 0.1060\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 12s 250us/step - loss: 2.2900 - acc: 0.1140 - val_loss: 2.2907 - val_acc: 0.1060\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 13s 263us/step - loss: 2.2896 - acc: 0.1141 - val_loss: 2.2905 - val_acc: 0.1060\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 12s 253us/step - loss: 2.2893 - acc: 0.1142 - val_loss: 2.2903 - val_acc: 0.1060\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 12s 253us/step - loss: 2.2889 - acc: 0.1141 - val_loss: 2.2898 - val_acc: 0.1060\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 13s 261us/step - loss: 2.2885 - acc: 0.1142 - val_loss: 2.2894 - val_acc: 0.10608s -  - ETA: 5s - loss: 2.2883 - a - ETA: 4s - loss: 2.2884 - acc: 0. - ETA: 4s - loss: 2.2883 - acc: - ETA: 3s - loss:  - ETA: 0s - loss: 2.2885 - acc: 0.114\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 12s 251us/step - loss: 2.2881 - acc: 0.1144 - val_loss: 2.2890 - val_acc: 0.1060 - ETA: 3s - ETA: 1s - loss: 2.2883 - a - ETA: 0s - loss: 2.2882 - ac\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 12s 250us/step - loss: 2.2877 - acc: 0.1176 - val_loss: 2.2887 - val_acc: 0.1060\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 12s 256us/step - loss: 2.2872 - acc: 0.1151 - val_loss: 2.2880 - val_acc: 0.1237\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 12s 251us/step - loss: 2.2868 - acc: 0.1205 - val_loss: 2.2877 - val_acc: 0.1060\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 12s 248us/step - loss: 2.2863 - acc: 0.1183 - val_loss: 2.2874 - val_acc: 0.1060\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 12s 251us/step - loss: 2.2859 - acc: 0.1185 - val_loss: 2.2867 - val_acc: 0.1060\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 12s 251us/step - loss: 2.2853 - acc: 0.1257 - val_loss: 2.2866 - val_acc: 0.1060\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 12s 259us/step - loss: 2.2848 - acc: 0.1237 - val_loss: 2.2855 - val_acc: 0.1263\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 12s 247us/step - loss: 2.2842 - acc: 0.1293 - val_loss: 2.2851 - val_acc: 0.1094\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 12s 251us/step - loss: 2.2836 - acc: 0.1313 - val_loss: 2.2844 - val_acc: 0.1172\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 12s 250us/step - loss: 2.2830 - acc: 0.1380 - val_loss: 2.2840 - val_acc: 0.1073838 - acc:  - ETA: 6s - loss: 2.2836 - acc: - ETA: 6s - loss: - ETA: 4s - loss - ETA: 3s - l - ETA:\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 13s 272us/step - loss: 2.2824 - acc: 0.1367 - val_loss: 2.2833 - val_acc: 0.1141\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 13s 277us/step - loss: 2.2817 - acc: 0.1430 - val_loss: 2.2826 - val_acc: 0.1124\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 11s 237us/step - loss: 2.2810 - acc: 0.1434 - val_loss: 2.2818 - val_acc: 0.1290\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 14s 291us/step - loss: 2.2802 - acc: 0.1483 - val_loss: 2.2809 - val_acc: 0.1425\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 2441s 51ms/step - loss: 2.2794 - acc: 0.1542 - val_loss: 2.2801 - val_acc: 0.1399\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 16s 343us/step - loss: 2.2785 - acc: 0.1565 - val_loss: 2.2793 - val_acc: 0.1522\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 15s 308us/step - loss: 2.2776 - acc: 0.1596 - val_loss: 2.2782 - val_acc: 0.1796\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 16s 325us/step - loss: 2.2767 - acc: 0.1659 - val_loss: 2.2773 - val_acc: 0.1853\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 12s 259us/step - loss: 2.2758 - acc: 0.1701 - val_loss: 2.2764 - val_acc: 0.1772\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 12s 256us/step - loss: 2.2747 - acc: 0.1766 - val_loss: 2.2757 - val_acc: 0.1455- l - ETA: 1s -\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 13s 261us/step - loss: 2.2737 - acc: 0.1770 - val_loss: 2.2748 - val_acc: 0.1387\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 12s 245us/step - loss: 2.2725 - acc: 0.1784 - val_loss: 2.2731 - val_acc: 0.1866\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 13s 262us/step - loss: 2.2713 - acc: 0.1851 - val_loss: 2.2722 - val_acc: 0.1567\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 13s 274us/step - loss: 2.2700 - acc: 0.1861 - val_loss: 2.2707 - val_acc: 0.1702\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 13s 272us/step - loss: 2.2686 - acc: 0.1890 - val_loss: 2.2691 - val_acc: 0.1942\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 12s 251us/step - loss: 2.2672 - acc: 0.1953 - val_loss: 2.2682 - val_acc: 0.1604\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 12s 241us/step - loss: 2.2657 - acc: 0.1927 - val_loss: 2.2665 - val_acc: 0.1793ss: 2.2657 - ac\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 12s 241us/step - loss: 2.2640 - acc: 0.1990 - val_loss: 2.2645 - val_acc: 0.19181 - ETA: 1s\n",
      "Epoch 100/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 12s 245us/step - loss: 2.2623 - acc: 0.1988 - val_loss: 2.2631 - val_acc: 0.1791\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 9936s 207ms/step - loss: 2.2605 - acc: 0.2004 - val_loss: 2.2607 - val_acc: 0.2175\n",
      "Epoch 102/200\n",
      "  512/48000 [..............................] - ETA: 27s - loss: 2.2584 - acc: 0.2207"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bunmi\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.164022). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 24s 493us/step - loss: 2.2586 - acc: 0.2058 - val_loss: 2.2588 - val_acc: 0.2004acc: - ETA: 1s - loss: 2.258\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 17s 353us/step - loss: 2.2563 - acc: 0.2075 - val_loss: 2.2573 - val_acc: 0.1831\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 15s 321us/step - loss: 2.2542 - acc: 0.2039 - val_loss: 2.2542 - val_acc: 0.2186\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 15s 312us/step - loss: 2.2519 - acc: 0.2140 - val_loss: 2.2521 - val_acc: 0.1979\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 15s 309us/step - loss: 2.2493 - acc: 0.2079 - val_loss: 2.2493 - val_acc: 0.2305\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 14s 298us/step - loss: 2.2466 - acc: 0.2155 - val_loss: 2.2466 - val_acc: 0.2131\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 14s 291us/step - loss: 2.2437 - acc: 0.2152 - val_loss: 2.2436 - val_acc: 0.2103ss: 2.2436 - acc:\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 17s 357us/step - loss: 2.2406 - acc: 0.2188 - val_loss: 2.2411 - val_acc: 0.1995\n",
      "Epoch 110/200\n",
      " 7680/48000 [===>..........................] - ETA: 24s - loss: 2.2381 - acc: 0.2229"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bunmi\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.155220). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8704/48000 [====>.........................] - ETA: 25s - loss: 2.2378 - acc: 0.2222"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bunmi\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.154664). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 20s 410us/step - loss: 2.2372 - acc: 0.2211 - val_loss: 2.2376 - val_acc: 0.2008\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 16s 328us/step - loss: 2.2337 - acc: 0.2193 - val_loss: 2.2339 - val_acc: 0.2084\n",
      "Epoch 112/200\n",
      "10368/48000 [=====>........................] - ETA: 28:50:19 - loss: 2.2289 - acc: 0.2252"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bunmi\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.120013). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 28625s 596ms/step - loss: 2.2297 - acc: 0.2214 - val_loss: 2.2295 - val_acc: 0.2402\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 643s 13ms/step - loss: 2.2256 - acc: 0.2264 - val_loss: 2.2255 - val_acc: 0.2160.2256 - acc: 0.2\n",
      "Epoch 114/200\n",
      "12544/48000 [======>.......................] - ETA: 35s - loss: 2.2231 - acc: 0.2180"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bunmi\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.186823). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46080/48000 [===========================>..] - ETA: 1s - loss: 2.2212 - acc: 0.2236"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bunmi\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.108372). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 39s 810us/step - loss: 2.2213 - acc: 0.2235 - val_loss: 2.2218 - val_acc: 0.2219\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 23s 485us/step - loss: 2.2164 - acc: 0.2276 - val_loss: 2.2166 - val_acc: 0.2281\n",
      "Epoch 116/200\n",
      "  512/48000 [..............................] - ETA: 24s - loss: 2.2251 - acc: 0.2227"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bunmi\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.119547). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 21s 434us/step - loss: 2.2113 - acc: 0.2330 - val_loss: 2.2107 - val_acc: 0.2327\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 20s 415us/step - loss: 2.2060 - acc: 0.2388 - val_loss: 2.2056 - val_acc: 0.2165\n",
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 15s 311us/step - loss: 2.2001 - acc: 0.2366 - val_loss: 2.1995 - val_acc: 0.2415\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 14s 284us/step - loss: 2.1938 - acc: 0.2379 - val_loss: 2.1933 - val_acc: 0.2383\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 14s 291us/step - loss: 2.1870 - acc: 0.2476 - val_loss: 2.1868 - val_acc: 0.2281 ETA: 1s - loss: 2\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 13s 261us/step - loss: 2.1798 - acc: 0.2484 - val_loss: 2.1784 - val_acc: 0.2437\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 13s 278us/step - loss: 2.1721 - acc: 0.2487 - val_loss: 2.1711 - val_acc: 0.2529\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 15s 305us/step - loss: 2.1638 - acc: 0.2545 - val_loss: 2.1623 - val_acc: 0.2715s - loss: 2.1635 - acc:\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 15s 319us/step - loss: 2.1550 - acc: 0.2619 - val_loss: 2.1532 - val_acc: 0.2609\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 13s 263us/step - loss: 2.1458 - acc: 0.2636 - val_loss: 2.1435 - val_acc: 0.2722\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 13s 263us/step - loss: 2.1359 - acc: 0.2722 - val_loss: 2.1340 - val_acc: 0.2808\n",
      "Epoch 127/200\n",
      "48000/48000 [==============================] - 13s 275us/step - loss: 2.1252 - acc: 0.2762 - val_loss: 2.1230 - val_acc: 0.2865\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 15s 315us/step - loss: 2.1144 - acc: 0.2817 - val_loss: 2.1120 - val_acc: 0.2903\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 14s 294us/step - loss: 2.1029 - acc: 0.2887 - val_loss: 2.1008 - val_acc: 0.2880ss: 2.1037 - acc - ETA: 2s  - ETA: 1s - loss: 2.\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 14s 282us/step - loss: 2.0914 - acc: 0.2928 - val_loss: 2.0895 - val_acc: 0.2856\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 13s 268us/step - loss: 2.0787 - acc: 0.2983 - val_loss: 2.0757 - val_acc: 0.3083A: 1s - loss:\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 14s 286us/step - loss: 2.0664 - acc: 0.3022 - val_loss: 2.0663 - val_acc: 0.2982\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 15s 306us/step - loss: 2.0536 - acc: 0.3134 - val_loss: 2.0497 - val_acc: 0.3196\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 13s 278us/step - loss: 2.0399 - acc: 0.3194 - val_loss: 2.0366 - val_acc: 0.3231\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 14s 300us/step - loss: 2.0268 - acc: 0.3255 - val_loss: 2.0229 - val_acc: 0.3308: 3s - loss: 2.029 - ETA: 2s - loss:  - ETA: 1s - loss: 2.02\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 15s 306us/step - loss: 2.0132 - acc: 0.3303 - val_loss: 2.0094 - val_acc: 0.3511\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 14s 294us/step - loss: 1.9998 - acc: 0.3392 - val_loss: 1.9996 - val_acc: 0.3314\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 15s 311us/step - loss: 1.9867 - acc: 0.3432 - val_loss: 1.9823 - val_acc: 0.3493\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 12s 254us/step - loss: 1.9736 - acc: 0.3470 - val_loss: 1.9851 - val_acc: 0.3193\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 13s 262us/step - loss: 1.9606 - acc: 0.3502 - val_loss: 1.9607 - val_acc: 0.3542\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 15s 303us/step - loss: 1.9480 - acc: 0.3611 - val_loss: 1.9433 - val_acc: 0.3809 1.9488 - acc\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 16s 329us/step - loss: 1.9354 - acc: 0.3685 - val_loss: 1.9303 - val_acc: 0.3828\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 14s 297us/step - loss: 1.9236 - acc: 0.3724 - val_loss: 1.9196 - val_acc: 0.3959\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 13s 271us/step - loss: 1.9109 - acc: 0.3856 - val_loss: 1.9160 - val_acc: 0.3657\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 12s 256us/step - loss: 1.9002 - acc: 0.3789 - val_loss: 1.8957 - val_acc: 0.3845oss: 1.9006 -\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 12s 249us/step - loss: 1.8900 - acc: 0.3894 - val_loss: 1.8839 - val_acc: 0.3940\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 12s 255us/step - loss: 1.8783 - acc: 0.3927 - val_loss: 1.8890 - val_acc: 0.3842\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 12s 251us/step - loss: 1.8687 - acc: 0.3960 - val_loss: 1.8940 - val_acc: 0.3714.8 - ETA: 4s - loss: 1.8 - ETA: 3s - ETA: 1s \n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 12s 251us/step - loss: 1.8588 - acc: 0.4034 - val_loss: 1.8586 - val_acc: 0.3968: 0s - loss: 1.8597 - acc: 0. - ETA: 0s - loss: 1.8592 - ac\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 12s 250us/step - loss: 1.8512 - acc: 0.4072 - val_loss: 1.8456 - val_acc: 0.4104\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 12s 252us/step - loss: 1.8425 - acc: 0.4097 - val_loss: 1.8312 - val_acc: 0.4370\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 12s 251us/step - loss: 1.8328 - acc: 0.4154 - val_loss: 1.8429 - val_acc: 0.4057\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 12s 250us/step - loss: 1.8253 - acc: 0.4160 - val_loss: 1.8174 - val_acc: 0.4649\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 12s 255us/step - loss: 1.8149 - acc: 0.4285 - val_loss: 1.8107 - val_acc: 0.4416813 - ET - E - ETA: 1s -\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 12s 252us/step - loss: 1.8083 - acc: 0.4346 - val_loss: 1.8348 - val_acc: 0.3898\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 12s 252us/step - loss: 1.8038 - acc: 0.4229 - val_loss: 1.7849 - val_acc: 0.4373\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 12s 258us/step - loss: 1.7923 - acc: 0.4313 - val_loss: 1.7757 - val_acc: 0.4742\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 12s 246us/step - loss: 1.7874 - acc: 0.4360 - val_loss: 1.7784 - val_acc: 0.4378\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 12s 247us/step - loss: 1.7802 - acc: 0.4356 - val_loss: 1.7812 - val_acc: 0.4636\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 12s 248us/step - loss: 1.7780 - acc: 0.4352 - val_loss: 1.7556 - val_acc: 0.4758loss: 1.7808 - acc: 0.43 - ETA\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 12s 254us/step - loss: 1.7694 - acc: 0.4381 - val_loss: 1.7905 - val_acc: 0.4391- acc: 0.43\n",
      "Epoch 162/200\n",
      "48000/48000 [==============================] - 13s 261us/step - loss: 1.7649 - acc: 0.4465 - val_loss: 1.7350 - val_acc: 0.4649\n",
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 12s 249us/step - loss: 1.7615 - acc: 0.4459 - val_loss: 1.7233 - val_acc: 0.4734 - ETA: 3s - - ETA: 1s - loss: 1.7676 - acc: 0 - ETA: 1s - loss: 1.76 - ETA: 0s - loss: 1.7642 - acc:\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 12s 252us/step - loss: 1.7438 - acc: 0.4540 - val_loss: 1.7182 - val_acc: 0.4703\n",
      "Epoch 165/200\n",
      "48000/48000 [==============================] - 12s 247us/step - loss: 1.7445 - acc: 0.4444 - val_loss: 1.7211 - val_acc: 0.4601\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 12s 245us/step - loss: 1.7460 - acc: 0.4526 - val_loss: 1.7066 - val_acc: 0.4728\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 12s 251us/step - loss: 1.7331 - acc: 0.4550 - val_loss: 1.6964 - val_acc: 0.4741\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 12s 241us/step - loss: 1.7414 - acc: 0.4521 - val_loss: 1.6765 - val_acc: 0.5035\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 12s 250us/step - loss: 1.7161 - acc: 0.4583 - val_loss: 1.6781 - val_acc: 0.4923\n",
      "Epoch 170/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 12s 257us/step - loss: 1.7085 - acc: 0.4644 - val_loss: 1.6615 - val_acc: 0.5038\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 12s 259us/step - loss: 1.6961 - acc: 0.4692 - val_loss: 1.6442 - val_acc: 0.5149\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 12s 257us/step - loss: 1.6954 - acc: 0.4655 - val_loss: 1.6469 - val_acc: 0.5227\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 12s 259us/step - loss: 1.6863 - acc: 0.4701 - val_loss: 1.6377 - val_acc: 0.4943\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 12s 255us/step - loss: 1.6702 - acc: 0.4736 - val_loss: 1.7136 - val_acc: 0.4385\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 12s 258us/step - loss: 1.6759 - acc: 0.4720 - val_loss: 1.6301 - val_acc: 0.4824loss: 1.6797 - a\n",
      "Epoch 176/200\n",
      "48000/48000 [==============================] - 12s 257us/step - loss: 1.6509 - acc: 0.4739 - val_loss: 1.6162 - val_acc: 0.4986\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 12s 250us/step - loss: 1.6335 - acc: 0.4832 - val_loss: 1.5714 - val_acc: 0.5261s - loss: 1.6401  - ETA: 2s - loss: 1.6365 - acc: 0.481 - ETA: 2s - loss: 1.6360 - acc: 0.48 - ETA: 2s - loss: 1.6360 -  - ETA: 1s -\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 12s 253us/step - loss: 1.6111 - acc: 0.4915 - val_loss: 1.5728 - val_acc: 0.5219 1.6120 - a - ETA: 0s - loss: 1.6141\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 12s 248us/step - loss: 1.5870 - acc: 0.5015 - val_loss: 1.5434 - val_acc: 0.5267 - ETA: 1s - loss: 1.\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 12s 256us/step - loss: 1.5943 - acc: 0.4887 - val_loss: 1.6488 - val_acc: 0.4736\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 12s 257us/step - loss: 1.5781 - acc: 0.4982 - val_loss: 1.5450 - val_acc: 0.5269s: 1.5962  - ETA: 2s - loss: 1.5 - ETA: 1s - \n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 12s 258us/step - loss: 1.5815 - acc: 0.4872 - val_loss: 1.5119 - val_acc: 0.5400\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 12s 247us/step - loss: 1.5254 - acc: 0.5102 - val_loss: 2.0844 - val_acc: 0.2239\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 12s 251us/step - loss: 1.5362 - acc: 0.5017 - val_loss: 1.5601 - val_acc: 0.4736\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 13s 268us/step - loss: 1.5078 - acc: 0.5118 - val_loss: 1.6556 - val_acc: 0.4378c: 0.5\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 12s 253us/step - loss: 1.5172 - acc: 0.5059 - val_loss: 1.5515 - val_acc: 0.4640\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 12s 247us/step - loss: 1.4797 - acc: 0.5114 - val_loss: 1.4741 - val_acc: 0.4982loss:  - ETA: 6s - loss:  - ETA: 5s - loss: 1.4936 - ac\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 12s 246us/step - loss: 1.4656 - acc: 0.5164 - val_loss: 1.4140 - val_acc: 0.5697\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 12s 253us/step - loss: 1.4456 - acc: 0.5241 - val_loss: 1.4934 - val_acc: 0.4667- acc: 0.52 - ETA: 0s - loss: 1.4466\n",
      "Epoch 190/200\n",
      "48000/48000 [==============================] - 12s 249us/step - loss: 1.4281 - acc: 0.5287 - val_loss: 1.3641 - val_acc: 0.5611\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 17s 358us/step - loss: 1.4013 - acc: 0.5323 - val_loss: 1.3787 - val_acc: 0.5328\n",
      "Epoch 192/200\n",
      "  384/48000 [..............................] - ETA: 48s - loss: 1.4460 - acc: 0.5495 ETA: 56s - loss: 1.4638 - acc: 0.55"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bunmi\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.156064). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Bunmi\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.112289). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1152/48000 [..............................] - ETA: 43s - loss: 1.3929 - acc: 0.5556"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bunmi\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.148211). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Bunmi\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.140358). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Bunmi\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.104437). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 15s 316us/step - loss: 1.3970 - acc: 0.5341 - val_loss: 1.3872 - val_acc: 0.5698\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 16s 324us/step - loss: 1.3692 - acc: 0.5401 - val_loss: 1.3380 - val_acc: 0.5757684 - acc: 0.540\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 12s 256us/step - loss: 1.3732 - acc: 0.5372 - val_loss: 1.3649 - val_acc: 0.5094\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 12s 258us/step - loss: 1.3465 - acc: 0.5449 - val_loss: 1.2976 - val_acc: 0.58930s - loss: 1.3405\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 12s 255us/step - loss: 1.3340 - acc: 0.5480 - val_loss: 1.2656 - val_acc: 0.5764 - loss: 1.3370 - a\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 12s 258us/step - loss: 1.3170 - acc: 0.5489 - val_loss: 1.3306 - val_acc: 0.5213\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 13s 262us/step - loss: 1.2927 - acc: 0.5557 - val_loss: 1.2362 - val_acc: 0.5865\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 12s 260us/step - loss: 1.2830 - acc: 0.5582 - val_loss: 1.2218 - val_acc: 0.5991\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 13s 264us/step - loss: 1.2628 - acc: 0.5636 - val_loss: 1.2134 - val_acc: 0.5943: 1.265\n",
      "10000/10000 [==============================] - ETA:  - 2s 236us/step\n",
      "test score:  1.2142494174957275\n",
      "test accuracy:  0.5888\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(n_hidden, input_shape=(reshaped,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(n_hidden))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
    "             metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size,\n",
    "                    epochs=nb_epoch, verbose=verbose, \n",
    "                   validation_split=validation_split)\n",
    "score = model.evaluate(x_test, y_test, verbose=verbose)\n",
    "\n",
    "print('test score: ', score[0])\n",
    "print('test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Futher improving the simle net with dropout also known as regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(n_hidden, input_shape=(reshaped,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(n_hidden))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.complie(loss='categorical_crossentorpy', optimizer=optimizer,\n",
    "             metrics=['accuracy'])\n",
    "history = modle.fit(x_train, y_train, batch_size=batch_size,\n",
    "                    epochs=nb_epoch, verbose=verbose, \n",
    "                   validation_split=validation_split)\n",
    "score = model.evaluate(x_test, y_test, verbose=verbose)\n",
    "\n",
    "print('test score: ', score[0])\n",
    "print('test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using keras optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop()\n",
    "\n",
    "# reduce number of epoch to 20\n",
    "# change the optimizer and run the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam()\n",
    "# reduce number of epoch to 20\n",
    "# change the optimizer and run the model again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: spending more time on training(increasing the number of epochs ) does not necessarily improve accuracy\n",
    "\n",
    "### the following can improve score\n",
    "increasing the number of internal hidden neurons\n",
    "contolling the optimizer learning rate can aslo improve the score\n",
    "\n",
    "This increase of\n",
    "complexity might have two negative consequences. First, a complex model might require a\n",
    "significant amount of time to be executed. Second, a complex model can achieve very good\n",
    "performance on training data—because all the inherent relations in trained data are\n",
    "memorized, but not so good performance on validation data—as the model is not able to\n",
    "generalize on fresh unseen data.\n",
    "\n",
    "There are three different types of regularizations used in machine learning:\n",
    "### L1 regularization (also known as lasso): \n",
    "The complexity of the model is\n",
    "expressed as the sum of the absolute values of the weights\n",
    "### L2 regularization (also known as ridge): \n",
    "The complexity of the model is\n",
    "expressed as the sum of the squares of the weights\n",
    "### Elastic net regularization: \n",
    "The complexity of the model is captured by a\n",
    "combination of the two preceding techniques\n",
    "\n",
    "Therefore, playing with regularization can be a good way to increase the performance of a\n",
    "network, in particular when there is an evident situation of overfitting. This set of\n",
    "experiments is left as an exercise for the interested reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "model.add(Dense(64, input_dim=64, kernel_regularizer = regularizers.l2(0.01)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.evaluate: This is used to compute the loss values\n",
    "### model.predict_classes: This is used to compute category outputs\n",
    "### model.predict_proba: This is used to compute class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir .\n",
    "# to open tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architectures can be easily saved and loaded as follows:\n",
    "\n",
    "save as JSON json_string = model.to_json()\n",
    "save as YAML yaml_string = model.to_yaml()\n",
    "\n",
    "\n",
    "model reconstruction from JSON: from keras.models import model_from_json\n",
    "\n",
    "\n",
    "model = model_from_json(json_string):\n",
    "\n",
    "\n",
    "model reconstruction from YAML model = model_from_yaml(yaml_string)\n",
    "\n",
    "### Model parameters (weights) can be easily saved and loaded as follows:\n",
    "\n",
    "from keras.models import load_model model.save('my_model.h5')\n",
    "\n",
    "\n",
    "creates a HDF5 file 'my_model\n",
    "\n",
    "\n",
    "deletes the existing model\n",
    "\n",
    "\n",
    "returns a compliled model\n",
    "\n",
    "\n",
    "identical to the previous one model = load_model('my_model.h5')\n",
    "\n",
    "### The training process can be stopped when a metric has stopped improving by using an appropriate callback:\n",
    "\n",
    "keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        model = Sequential()\n",
    "        model.add(Dense(10, input_dim=784, init='uniform'))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer = 'rmsprop')\n",
    "        \n",
    "history = LossHistory()\n",
    "model.fit(x_train, y_train, batch_size=128, nb_epoch=20, verbose=0,\n",
    "         callbacks=[history])\n",
    "print(history.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpointing\n",
    "Checkpointing is a process that saves a snapshot of the application's state at regular\n",
    "intervals, so the application can be restarted from the last saved state in case of failure. This\n",
    "is useful during training of deep learning models, which can often be a time-consuming\n",
    "task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.datasets import minst\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "batch_size = 128\n",
    "num_epochs = 20\n",
    "model_dir = \"/tmp\"\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(60000, 784).astype(\"float32\") / 255\n",
    "y_train = np.utils.to_categorical(y_train, 10)\n",
    "y_test = np.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,), activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model\n",
    "checkpoint = ModelCheckpoint(filepath=os.path.join(model_dir, 'model-{epoch:02d}.h5'), save_best_only=True)\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, nb_epoch=num_epochs,\n",
    "         validation_split=0.1, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides a callback for saving your training and test metrics, as well as activation\n",
    "histograms for the different layers in your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.callbacks.Tensorboard(log_dir='./logs', histogram_freq=0,\n",
    "                           write_graph=True, write_images=False)\n",
    "# saved data can be visualized with tensorboard launched at the command line:\n",
    "tensorboard --logdir=/full_path_to_your_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 29s 3us/step\n",
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 444s 3us/step\n",
      "Collecting quiver_engine"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\Bunmi\\OneDrive\\Documents\\envs\\newbot\\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Bunmi\\\\AppData\\\\Local\\\\Temp\\\\pip-install-4ksflvqy\\\\quiver-engine\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Bunmi\\\\AppData\\\\Local\\\\Temp\\\\pip-install-4ksflvqy\\\\quiver-engine\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\Bunmi\\AppData\\Local\\Temp\\pip-install-4ksflvqy\\quiver-engine\\pip-egg-info'\n",
      "         cwd: C:\\Users\\Bunmi\\AppData\\Local\\Temp\\pip-install-4ksflvqy\\quiver-engine\\\n",
      "    Complete output (1 lines):\n",
      "    error in quiver_engine setup command: \"values of 'package_data' dict\" must be a list of strings (got 'quiverboard/dist/*')\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading quiver_engine-0.1.4.1.4.tar.gz (398 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement opencv (from versions: none)\n",
      "ERROR: No matching distribution found for opencv\n"
     ]
    }
   ],
   "source": [
    "# installing quiver for visualizing convnets features\n",
    "from keras.datasets import cifar10, mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "!pip install quiver_engine\n",
    "!pip install opencv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quiver_engine import server\n",
    "server.launch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNets\n",
    "\n",
    "There are three key intuitions beyond ConvNets:\n",
    "\n",
    "\n",
    "1. Local receptive fields\n",
    "\n",
    "\n",
    "2. Shared weights\n",
    "\n",
    "\n",
    "3. Pooling\n",
    "\n",
    "Assuming that the input image has shape (256, 256) on three channels with tf (TensorFlow)\n",
    "ordering, this is represented as (256, 256, 3). Note that with th (Theano) mode, the channel's\n",
    "dimension (the depth) is at index 1; in tf (TensoFlow) mode, it is at index 3.\n",
    "\n",
    "\n",
    "In Keras, if we want to add a convolutional layer with dimensionality of the output 32 and\n",
    "extension of each filter 3 x 3, we will write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(256, 256, 3)))\n",
    "# or we can write the second line as\n",
    "# model.add(Conv2D(32, kernel_size=3, input_shape=(256, 256, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max pooling of size 2 x 2\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another choice is average pooling which aggregates the average activation values in that region\n",
    "# more info at https://keras.io/layers/pooling\n",
    "# all pooling otions are nothing more than a summary operation on a given region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs apply convolution and\n",
    "pooling operations <b>in one dimension for audio and text data</b> along the time dimension, in\n",
    "<b>two dimensions for images along the (height x width) dimensions</b>, and in <b>three dimensions\n",
    "for videos along the (height x width x time) dimensions.</b>\n",
    "\n",
    "\n",
    "LeNet , a family of ConvNet trained for recognizing mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.convolutional.Conv2D(filters, kernel_size, padding ='valid')\n",
    "# filters - no of convolutional kernels to use. e.g dimensionality of the output\n",
    "# kernel size is an integer or tuple/list specifying the width and height od 2D conv. window\n",
    "# There are two options: padding ='valid' means that the convolution is only computed where the\n",
    "#input and the filter fully overlap, and therefore the output is smaller than the input, while\n",
    "# padding='same' means that we have an output that is the same size as the input, for which\n",
    "# the area around the input is padded with zeros.\n",
    "\n",
    "keras.layers.pooling.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "# Here, pool_size is a tuple of two integers representing the factors by which the\n",
    "# image is vertically and horizontally downscaled. So (2, 2) will halve the image in each\n",
    "# dimension, and strides=(2, 2) is the stride used for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's code this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import minst\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Flatten, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as k\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the ConvNet\n",
    "class LeNet:\n",
    "    @staticmethod\n",
    "    def build(input_shape, classes):\n",
    "        model = Sequential()\n",
    "        # conv => relu => pool\n",
    "        model.add(Convolution2D(20, kernel_size=5, padding='same', \n",
    "                               input_shape=input_shape))\n",
    "        model.add(Convolution2D(20, kernel_size=5, padding='same',\n",
    "                        input_shape=input_shape))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model.add(Convolution2D(50, kernel_size=5, padding='same',\n",
    "                                input_shape=input_shape))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(500))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation('softmax'))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 20\n",
    "batch_size = 128\n",
    "verbose = 1\n",
    "optimizer = 1\n",
    "optimizer = Adam()\n",
    "validation_split = 0.2\n",
    "img_rows, img_cols = 28, 28\n",
    "nb_classes = 10\n",
    "input_shape = (1, img_rows, img_cols)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "k.set_image_dim_ordering('th')\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "x_train = x_train[:, np.newaxis, :, :]\n",
    "x_test = x_test[:, np.newaxis, :, :]\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "model = LeNet.build(input_shape = input_shape, classes=nb_classes)\n",
    "model.complie(loss='categorical_crossentropy', optimizer = optimizer,\n",
    "              metrics = ['accuracy'])\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "                   verbose=verbose, validation_split=validation_split)\n",
    "score = model.evaluate(x_test, y_test, verbose=verbose)\n",
    "\n",
    "print('test score: ', score[0])\n",
    "print('test accuracy: ', score[1])\n",
    "\n",
    "print(history.history.keys())\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying cifar10 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Bunmi\\OneDrive\\Documents\\envs\\newbot\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Bunmi\\OneDrive\\Documents\\envs\\newbot\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Bunmi\\OneDrive\\Documents\\envs\\newbot\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Bunmi\\OneDrive\\Documents\\envs\\newbot\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Bunmi\\OneDrive\\Documents\\envs\\newbot\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Bunmi\\OneDrive\\Documents\\envs\\newbot\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.layers.core.convolutional'; 'keras.layers.core' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d638d92156db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvolutional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSGD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRMSprop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras.layers.core.convolutional'; 'keras.layers.core' is not a package"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.core.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar10 is a set of 60k images 32 x 32 pixels on 3 channels\n",
    "img_channels=3\n",
    "img_cols, img_rows = 32, 32\n",
    "\n",
    "batch_size = 128\n",
    "nb_epoch =20\n",
    "nb_classes = 10\n",
    "verbose = 1\n",
    "validation_split = 0.2\n",
    "optim = RMSprop()\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "# to categorical\n",
    "y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "# float and normalization\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                input_shape = (img_rows, img_cols, img_channels)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "# train\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optim,\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size,epochs=nb_epoch,\n",
    "         validation_split=validation_split, verbose=verbose)\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size,\n",
    "                      verbose=verbose)\n",
    "print('test score: '), score[0]\n",
    "print('test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "model_json = model.to_json()\n",
    "open('cifar10_architecture.json', 'w').write(model.json)\n",
    "# weights\n",
    "model.save_weights('cifar10_weights.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### improving with a deeper net\n",
    "\n",
    "conv+conv+maxpool+dropout+conv+conv+maxpool followed by dense +dropout+dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                input_shape = (img_rows, img_cols, img_channels)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### improving the cifar10 performance with data augumentation\n",
    "i.e generating more images using various transformation types like horizontal/vertical flip, zooming, channel shift\n",
    "### rotation_range:\n",
    "a value in degrees between 0-180 for randomly rotating pictures\n",
    "### width and height shift ranges:\n",
    "randomly translating pictures horizontally and vertically\n",
    "### zoom_range:\n",
    "randomly zooming images\n",
    "### horizontal_flip:\n",
    "randomly flipping half of the images horizontally\n",
    "### fill_mode:\n",
    "strategy for filling in new pixels that can appear after a rotation or a shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "num_to_augument = 5\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('Augumenting training set images...')\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=40,\n",
    "                            width_shift_range = 0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode='nearest')\n",
    "xtas, ytas = [], []\n",
    "for i in range(x_train.shape[0]):\n",
    "    num_aug = 0\n",
    "    x = x_train[i] # (3, 32, 32)\n",
    "    x = x.reshape((1,) + x.reshape) # (1, 3, 32, 32)\n",
    "    for x_aug in datagen.flow(x, batch_size=1, save_to_dir='preview',\n",
    "                         save_prefix='cifar', save_format='jpeg'):\n",
    "        if num_aug >= num_to_augment:\n",
    "            break\n",
    "        xtas.append(x_aug[0])\n",
    "        num_aug += 1\n",
    "        \n",
    "# fit the datagen\n",
    "datagen.fit(x_train)\n",
    "#train\n",
    "history = model.fit_generator(datagen.flow(x_train, y_train, \n",
    "                                          batch_size=batch_size)\n",
    "                                          , samples_per_epoch=x_train.shape[0],\n",
    "                                          epochs=nb_epoch, verbose=verbose)\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size,\n",
    "                      verbose=verbose)\n",
    "print('test score: ', score[0])\n",
    "print('test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predicting with cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.misc\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# load model\n",
    "model_archi = 'cifar10_architecture.json'\n",
    "model_weights = 'cifar10_weights.h5'\n",
    "model = model_from_json(open(model_archi).read())\n",
    "model.load_weights(model_weights)\n",
    "\n",
    "# load images\n",
    "img_names = ['cat-standing.jpg', 'dog.jpg']\n",
    "imgs = [np.transpose(scipy.misc.imresize(scipy.misc.imread(img_name),\n",
    "                                        (32, 32)), (1, 0, 2)).astype('float32') for img_name in img_names]\n",
    "imgs = np.array(imgs) / 255\n",
    "\n",
    "# train\n",
    "optim = SGD()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "\n",
    "# predict\n",
    "predictions = model.predict_classes(imgs)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very deep convolutional networks for largescale image recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Flatten, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "import cv2, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a VGG16 network\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1, 1),\n",
    "                input_shape = (3, 224, 224)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "\n",
    "# code here\n",
    "#https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras built in vgg-16 net module\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.optimizers import SGD\n",
    "import cv2, numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "sgd = SGD(lr=0.1, decay=0.000001, momentum=0.9, nesterov=True)\n",
    "model.complie(optimizer=sgd, loss='categorical_crossentropy')\n",
    "\n",
    "im = cv2.resize(cv2.imread('steam-locomotive.jpg'), (224, 224))\n",
    "im = np.expand_dims(im, axis=0)\n",
    "\n",
    "out = model.predict(im)\n",
    "plt.plot(out.ravel())\n",
    "plt.show()\n",
    "print(np.argmax(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.model import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.preprocessing import image\n",
    "\n",
    "base_model = InceptionV3(weights='imagenet', include_topFalse)\n",
    "#\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "predictions = Dense(200, acrivation='softmax')(x)\n",
    "model = Model(input=base_model.input, output=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable=False\n",
    "model.complie(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "for layer in model.layers[:172]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[:172]:\n",
    "    layer.trainable = True\n",
    "from keras.optimizers import SGD\n",
    "model.complie(optimizer=SGD(lr=0.001, momentum=0.9),\n",
    "              loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep convolutional generative adversarial networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=100, output_dim=1024))\n",
    "    model.add(Actvation('tanh'))\n",
    "    model.add(Dense(128*7*7))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Reshape((128, 7,7), input_shape=(128*7*7,)))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Convolution2D(64, 5, 5, border_mode='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Convolution2D(1, 5, 5, border_mode='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    return model\n",
    "def discriminator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(64, 5, 5, border_mode='same', \n",
    "                           input_shape=(1, 28, 28)))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Convolution2D(128, 5, 5))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
